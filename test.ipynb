{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6fd7c463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing dependencies...\n"
     ]
    }
   ],
   "source": [
    "print(\"Installing dependencies...\")\n",
    "!pip install -q transformers torch sentence-transformers datasets pandas scikit-learn langdetect\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from google.colab import drive, files\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "from typing import List, Dict, Tuple\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf34a60d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "DATASET LOADING OPTIONS\n",
      "============================================================\n",
      "\n",
      "Choose how to load your dataset:\n",
      "1. Upload CSV file\n",
      "2. Upload JSON file\n",
      "3. Load from Google Drive\n",
      "4. Use sample data for testing\n",
      "5. Generate synthetic multilingual data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Using sample data\n",
      "\n",
      "ðŸ“Š Generating 100 synthetic samples...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8d88e92ac874310b886421bcb9a67f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating data:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Generated 100 samples\n",
      "   Positive pairs: 70 (70.0%)\n",
      "   Negative pairs: 30 (30.0%)\n",
      "   Languages: ['en', 'zh', 'fr', 'it', 'es', 'de']\n",
      "   Domains: ['information', 'travel', 'shopping']\n",
      "\n",
      "============================================================\n",
      "DATASET OVERVIEW\n",
      "============================================================\n",
      "\n",
      "Total samples: 100\n",
      "\n",
      "First few rows:\n",
      "                 query                       doc  label query_lang doc_lang  \\\n",
      "0  where to find learn                  ä»€ä¹ˆæ˜¯learn      1         en       zh   \n",
      "1             ä¸ºä»€ä¹ˆlearn                   å¦‚ä½•learn      1         zh       zh   \n",
      "2     flights to Paris    hoteles cerca de Paris      1         en       es   \n",
      "3          ä¸ºä»€ä¹ˆexercise              ä»€ä¹ˆæ—¶å€™exercise      1         zh       zh   \n",
      "4     when to meditate  dÃ³nde encontrar meditate      1         en       es   \n",
      "\n",
      "        domain  \n",
      "0  information  \n",
      "1  information  \n",
      "2       travel  \n",
      "3  information  \n",
      "4  information  \n",
      "\n",
      "Dataset statistics:\n",
      "                                 query           doc       label query_lang  \\\n",
      "count                              100           100  100.000000        100   \n",
      "unique                              95            96         NaN          6   \n",
      "top     attrazioni turistiche a Mumbai  comment cook         NaN         es   \n",
      "freq                                 2             2         NaN         20   \n",
      "mean                               NaN           NaN    0.700000        NaN   \n",
      "std                                NaN           NaN    0.460566        NaN   \n",
      "min                                NaN           NaN    0.000000        NaN   \n",
      "25%                                NaN           NaN    0.000000        NaN   \n",
      "50%                                NaN           NaN    1.000000        NaN   \n",
      "75%                                NaN           NaN    1.000000        NaN   \n",
      "max                                NaN           NaN    1.000000        NaN   \n",
      "\n",
      "       doc_lang       domain  \n",
      "count       100          100  \n",
      "unique        6            3  \n",
      "top          zh  information  \n",
      "freq         24           40  \n",
      "mean        NaN          NaN  \n",
      "std         NaN          NaN  \n",
      "min         NaN          NaN  \n",
      "25%         NaN          NaN  \n",
      "50%         NaN          NaN  \n",
      "75%         NaN          NaN  \n",
      "max         NaN          NaN  \n",
      "\n",
      "Label distribution:\n",
      "label\n",
      "1    70\n",
      "0    30\n",
      "Name: count, dtype: int64\n",
      " Labels are binary (0/1)\n",
      "\n",
      "ðŸ§¹ Cleaning data...\n",
      "   Removed 0 invalid rows\n",
      "   Final dataset size: 100\n",
      "\n",
      " Cleaned dataset saved to: /content/cleaned_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DATASET LOADING OPTIONS\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nChoose how to load your dataset:\")\n",
    "print(\"1. Upload CSV file\")\n",
    "print(\"2. Upload JSON file\")\n",
    "print(\"3. Load from Google Drive\")\n",
    "print(\"4. Use sample data for testing\")\n",
    "print(\"5. Generate synthetic multilingual data\")\n",
    "\n",
    "dataset_choice = input(\"\\nEnter choice (1-5): \").strip()\n",
    "\n",
    "def load_csv_dataset(file_path):\n",
    "    \"\"\"Load dataset from CSV\"\"\"\n",
    "    df = pd.read_csv(file_path)\n",
    "    print(f\"\\n Loaded CSV with {len(df)} rows\")\n",
    "    print(f\"   Columns: {list(df.columns)}\")\n",
    "    return df\n",
    "\n",
    "def load_json_dataset(file_path):\n",
    "   \n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    if isinstance(data, list):\n",
    "        df = pd.DataFrame(data)\n",
    "    else:\n",
    "        df = pd.DataFrame([data])\n",
    "    \n",
    "    print(f\"\\n Loaded JSON with {len(df)} rows\")\n",
    "    print(f\"   Columns: {list(df.columns)}\")\n",
    "    return df\n",
    "\n",
    "def generate_synthetic_data(num_samples=1000):\n",
    "   \n",
    "    print(f\"\\nðŸ“Š Generating {num_samples} synthetic samples...\")\n",
    "\n",
    "    templates = {\n",
    "        'travel': {\n",
    "            'en': ['cheap hotels in {}', 'flights to {}', 'tourist attractions in {}', \n",
    "                   'restaurants in {}', 'hotels near {}'],\n",
    "            'es': ['hoteles baratos en {}', 'vuelos a {}', 'atracciones turÃ­sticas en {}',\n",
    "                   'restaurantes en {}', 'hoteles cerca de {}'],\n",
    "            'fr': ['hÃ´tels bon marchÃ© Ã  {}', 'vols vers {}', 'attractions touristiques Ã  {}',\n",
    "                   'restaurants Ã  {}', 'hÃ´tels prÃ¨s de {}'],\n",
    "            'de': ['gÃ¼nstige Hotels in {}', 'FlÃ¼ge nach {}', 'Touristenattraktionen in {}',\n",
    "                   'Restaurants in {}', 'Hotels in der NÃ¤he von {}'],\n",
    "            'it': ['hotel economici a {}', 'voli per {}', 'attrazioni turistiche a {}',\n",
    "                   'ristoranti a {}', 'hotel vicino a {}'],\n",
    "            'zh': ['{}çš„ä¾¿å®œé…’åº—', 'é£žå¾€{}', '{}çš„æ—…æ¸¸æ™¯ç‚¹', '{}çš„é¤åŽ…', '{}é™„è¿‘çš„é…’åº—'],\n",
    "        },\n",
    "        'shopping': {\n",
    "            'en': ['buy {} online', 'best {} deals', '{} for sale', 'cheap {}', 'discount {}'],\n",
    "            'es': ['comprar {} en lÃ­nea', 'mejores ofertas de {}', '{} en venta', '{} barato', '{} con descuento'],\n",
    "            'fr': ['acheter {} en ligne', 'meilleures offres {}', '{} Ã  vendre', '{} pas cher', '{} en promotion'],\n",
    "            'de': ['{} online kaufen', 'beste {} Angebote', '{} zu verkaufen', 'gÃ¼nstige {}', '{} im Angebot'],\n",
    "            'it': ['acquista {} online', 'migliori offerte {}', '{} in vendita', '{} economico', '{} in sconto'],\n",
    "            'zh': ['åœ¨çº¿è´­ä¹°{}', 'æœ€å¥½çš„{}ä¼˜æƒ ', '{}å‡ºå”®', 'ä¾¿å®œçš„{}', '{}æŠ˜æ‰£'],\n",
    "        },\n",
    "        'information': {\n",
    "            'en': ['how to {}', 'what is {}', 'where to find {}', 'when to {}', 'why {}'],\n",
    "            'es': ['cÃ³mo {}', 'quÃ© es {}', 'dÃ³nde encontrar {}', 'cuÃ¡ndo {}', 'por quÃ© {}'],\n",
    "            'fr': ['comment {}', 'qu\\'est-ce que {}', 'oÃ¹ trouver {}', 'quand {}', 'pourquoi {}'],\n",
    "            'de': ['wie man {}', 'was ist {}', 'wo finde ich {}', 'wann {}', 'warum {}'],\n",
    "            'it': ['come {}', 'cos\\'Ã¨ {}', 'dove trovare {}', 'quando {}', 'perchÃ© {}'],\n",
    "            'zh': ['å¦‚ä½•{}', 'ä»€ä¹ˆæ˜¯{}', 'åœ¨å“ªé‡Œæ‰¾åˆ°{}', 'ä»€ä¹ˆæ—¶å€™{}', 'ä¸ºä»€ä¹ˆ{}'],\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    cities = ['Paris', 'London', 'Tokyo', 'New York', 'Rome', 'Berlin', 'Madrid', \n",
    "              'Barcelona', 'Amsterdam', 'Dubai', 'Singapore', 'Mumbai', 'Beijing']\n",
    "    \n",
    "    products = ['laptop', 'phone', 'camera', 'shoes', 'watch', 'book', 'tablet', \n",
    "                'headphones', 'backpack', 'jacket']\n",
    "    \n",
    "    topics = ['cook', 'learn', 'travel', 'exercise', 'meditate', 'study', 'work', \n",
    "              'relax', 'save money', 'start business']\n",
    "    \n",
    "    data = []\n",
    "    languages = list(templates['travel'].keys())\n",
    "    \n",
    "    for _ in tqdm(range(num_samples), desc=\"Generating data\"):\n",
    "        domain = random.choice(list(templates.keys()))\n",
    "        \n",
    "      \n",
    "        if domain == 'travel':\n",
    "            entity = random.choice(cities)\n",
    "        elif domain == 'shopping':\n",
    "            entity = random.choice(products)\n",
    "        else:\n",
    "            entity = random.choice(topics)\n",
    "        \n",
    "      \n",
    "        lang1 = random.choice(languages)\n",
    "        lang2 = random.choice(languages)\n",
    "        \n",
    "        template1 = random.choice(templates[domain][lang1])\n",
    "        template2 = random.choice(templates[domain][lang2])\n",
    "        \n",
    "        query = template1.format(entity)\n",
    "        \n",
    "\n",
    "        if random.random() < 0.7:\n",
    "            doc = template2.format(entity)\n",
    "            label = 1\n",
    "        else:\n",
    "    \n",
    "            if random.random() < 0.5:\n",
    "       \n",
    "                other_entity = random.choice(cities if domain == 'travel' else \n",
    "                                            products if domain == 'shopping' else topics)\n",
    "                while other_entity == entity:\n",
    "                    other_entity = random.choice(cities if domain == 'travel' else \n",
    "                                                products if domain == 'shopping' else topics)\n",
    "                doc = template2.format(other_entity)\n",
    "            else:\n",
    "              \n",
    "                other_domain = random.choice([d for d in templates.keys() if d != domain])\n",
    "                other_template = random.choice(templates[other_domain][lang2])\n",
    "                other_entity = random.choice(cities if other_domain == 'travel' else \n",
    "                                            products if other_domain == 'shopping' else topics)\n",
    "                doc = other_template.format(other_entity)\n",
    "            label = 0\n",
    "        \n",
    "        data.append({\n",
    "            'query': query,\n",
    "            'doc': doc,\n",
    "            'label': label,\n",
    "            'query_lang': lang1,\n",
    "            'doc_lang': lang2,\n",
    "            'domain': domain\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    print(f\"\\n Generated {len(df)} samples\")\n",
    "    print(f\"   Positive pairs: {(df['label']==1).sum()} ({(df['label']==1).sum()/len(df)*100:.1f}%)\")\n",
    "    print(f\"   Negative pairs: {(df['label']==0).sum()} ({(df['label']==0).sum()/len(df)*100:.1f}%)\")\n",
    "    print(f\"   Languages: {df['query_lang'].unique().tolist()}\")\n",
    "    print(f\"   Domains: {df['domain'].unique().tolist()}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "if dataset_choice == '1':\n",
    "    print(\"\\n Upload your CSV file\")\n",
    "    print(\"Expected format: columns [query, doc, label]\")\n",
    "    print(\"  - query: search query text\")\n",
    "    print(\"  - doc: document text\")\n",
    "    print(\"  - label: 1 (similar) or 0 (not similar)\")\n",
    "    uploaded = files.upload()\n",
    "    file_path = list(uploaded.keys())[0]\n",
    "    df = load_csv_dataset(file_path)\n",
    "    \n",
    "elif dataset_choice == '2':\n",
    "    print(\"\\n Upload your JSON file\")\n",
    "    print(\"Expected format: list of {query, doc, label}\")\n",
    "    uploaded = files.upload()\n",
    "    file_path = list(uploaded.keys())[0]\n",
    "    df = load_json_dataset(file_path)\n",
    "    \n",
    "elif dataset_choice == '3':\n",
    "    file_path = input(\"\\nEnter Google Drive path (e.g., /content/drive/MyDrive/data.csv): \").strip()\n",
    "    if file_path.endswith('.csv'):\n",
    "        df = load_csv_dataset(file_path)\n",
    "    elif file_path.endswith('.json'):\n",
    "        df = load_json_dataset(file_path)\n",
    "    else:\n",
    "        print(\"Unsupported file format. Using sample data.\")\n",
    "        df = generate_synthetic_data(100)\n",
    "        \n",
    "elif dataset_choice == '5':\n",
    "    num_samples = int(input(\"\\nHow many samples to generate? (default: 1000): \").strip() or \"1000\")\n",
    "    df = generate_synthetic_data(num_samples)\n",
    "    \n",
    "else: \n",
    "    print(\"\\n Using sample data\")\n",
    "    df = generate_synthetic_data(100)\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DATASET OVERVIEW\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nTotal samples: {len(df)}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(df.head())\n",
    "\n",
    "print(f\"\\nDataset statistics:\")\n",
    "print(df.describe(include='all'))\n",
    "\n",
    "\n",
    "required_cols = ['query', 'doc', 'label']\n",
    "missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "\n",
    "if missing_cols:\n",
    "    print(f\"\\n  Missing columns: {missing_cols}\")\n",
    "    print(\"Attempting to auto-detect columns...\")\n",
    "    \n",
    "  \n",
    "    for col in missing_cols:\n",
    "        possible_names = [c for c in df.columns if col.lower() in c.lower()]\n",
    "        if possible_names:\n",
    "            df.rename(columns={possible_names[0]: col}, inplace=True)\n",
    "            print(f\"   Renamed '{possible_names[0]}' â†’ '{col}'\")\n",
    "\n",
    "\n",
    "if 'label' in df.columns:\n",
    "    unique_labels = df['label'].unique()\n",
    "    print(f\"\\nLabel distribution:\")\n",
    "    print(df['label'].value_counts())\n",
    "    \n",
    "\n",
    "    if set(unique_labels).issubset({0, 1}):\n",
    "        print(\" Labels are binary (0/1)\")\n",
    "    else:\n",
    "        print(f\"Found labels: {unique_labels}\")\n",
    "        print(\"Converting to binary (0/1)...\")\n",
    "        df['label'] = df['label'].apply(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "\n",
    "print(\"\\nðŸ§¹ Cleaning data...\")\n",
    "initial_len = len(df)\n",
    "df = df.dropna(subset=['query', 'doc', 'label'])\n",
    "df = df[df['query'].str.strip() != '']\n",
    "df = df[df['doc'].str.strip() != '']\n",
    "print(f\"   Removed {initial_len - len(df)} invalid rows\")\n",
    "print(f\"   Final dataset size: {len(df)}\")\n",
    "\n",
    "\n",
    "cleaned_path = '/content/cleaned_dataset.csv'\n",
    "df.to_csv(cleaned_path, index=False)\n",
    "print(f\"\\n Cleaned dataset saved to: {cleaned_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7d006928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "DATA SPLITTING\n",
      "============================================================\n",
      "\n",
      "Split ratios:\n",
      "  Train: 70.0%\n",
      "  Validation: 15.0%\n",
      "  Test: 15.0%\n",
      "\n",
      "Dataset sizes:\n",
      "  Train: 69 samples\n",
      "  Validation: 15 samples\n",
      "  Test: 16 samples\n",
      "\n",
      "Label distribution:\n",
      "  Train - Positive: 48, Negative: 21\n",
      "  Val   - Positive: 11, Negative: 4\n",
      "  Test  - Positive: 11, Negative: 5\n",
      "\n",
      " Data splits saved\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DATA SPLITTING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "\n",
    "train_ratio = 0.7\n",
    "val_ratio = 0.15\n",
    "test_ratio = 0.15\n",
    "\n",
    "print(f\"\\nSplit ratios:\")\n",
    "print(f\"  Train: {train_ratio*100}%\")\n",
    "print(f\"  Validation: {val_ratio*100}%\")\n",
    "print(f\"  Test: {test_ratio*100}%\")\n",
    "\n",
    "\n",
    "train_df, temp_df = train_test_split(\n",
    "    df, \n",
    "    test_size=(1 - train_ratio),\n",
    "    stratify=df['label'],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "val_df, test_df = train_test_split(\n",
    "    temp_df,\n",
    "    test_size=test_ratio/(test_ratio + val_ratio),\n",
    "    stratify=temp_df['label'],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"\\nDataset sizes:\")\n",
    "print(f\"  Train: {len(train_df)} samples\")\n",
    "print(f\"  Validation: {len(val_df)} samples\")\n",
    "print(f\"  Test: {len(test_df)} samples\")\n",
    "\n",
    "\n",
    "print(f\"\\nLabel distribution:\")\n",
    "print(f\"  Train - Positive: {(train_df['label']==1).sum()}, Negative: {(train_df['label']==0).sum()}\")\n",
    "print(f\"  Val   - Positive: {(val_df['label']==1).sum()}, Negative: {(val_df['label']==0).sum()}\")\n",
    "print(f\"  Test  - Positive: {(test_df['label']==1).sum()}, Negative: {(test_df['label']==0).sum()}\")\n",
    "\n",
    "\n",
    "train_df.to_csv('/content/train.csv', index=False)\n",
    "val_df.to_csv('/content/val.csv', index=False)\n",
    "test_df.to_csv('/content/test.csv', index=False)\n",
    "\n",
    "print(\"\\n Data splits saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "618aaf16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Model class defined\n"
     ]
    }
   ],
   "source": [
    "class BiEncoderModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2', \n",
    "                 pooling='mean'):\n",
    "        super().__init__()\n",
    "        \n",
    "        print(f\"Loading model: {model_name}\")\n",
    "        self.encoder = AutoModel.from_pretrained(model_name)\n",
    "        self.pooling = pooling\n",
    "        self.config = self.encoder.config\n",
    "        \n",
    "        print(f\"âœ… Model loaded\")\n",
    "        print(f\"   Hidden size: {self.config.hidden_size}\")\n",
    "        print(f\"   Pooling strategy: {pooling}\")\n",
    "        \n",
    "    def mean_pooling(self, token_embeddings, attention_mask):\n",
    "        \n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "        sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "        return sum_embeddings / sum_mask\n",
    "    \n",
    "    def cls_pooling(self, token_embeddings):\n",
    "     \n",
    "        return token_embeddings[:, 0, :]\n",
    "    \n",
    "    def max_pooling(self, token_embeddings, attention_mask):\n",
    "  \n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        token_embeddings[input_mask_expanded == 0] = -1e9\n",
    "        return torch.max(token_embeddings, 1)[0]\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "     \n",
    "        \n",
    "        outputs = self.encoder(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            return_dict=True\n",
    "        )\n",
    "        \n",
    "       \n",
    "        if self.pooling == 'mean':\n",
    "            embeddings = self.mean_pooling(outputs.last_hidden_state, attention_mask)\n",
    "        elif self.pooling == 'cls':\n",
    "            embeddings = self.cls_pooling(outputs.last_hidden_state)\n",
    "        elif self.pooling == 'max':\n",
    "            embeddings = self.max_pooling(outputs.last_hidden_state, attention_mask)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown pooling: {self.pooling}\")\n",
    "        \n",
    "      \n",
    "        embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)\n",
    "        \n",
    "        return embeddings\n",
    "\n",
    "print(\"\\n Model class defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c5a6c4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultilingualSearchDataset(Dataset):\n",
    "   \n",
    "    \n",
    "    def __init__(self, dataframe, tokenizer, max_length=128):\n",
    "        self.data = dataframe.reset_index(drop=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        \n",
    "      \n",
    "        query_encoding = self.tokenizer(\n",
    "            str(row['query']),\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "       \n",
    "        doc_encoding = self.tokenizer(\n",
    "            str(row['doc']),\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'query_input_ids': query_encoding['input_ids'].squeeze(0),\n",
    "            'query_attention_mask': query_encoding['attention_mask'].squeeze(0),\n",
    "            'doc_input_ids': doc_encoding['input_ids'].squeeze(0),\n",
    "            'doc_attention_mask': doc_encoding['attention_mask'].squeeze(0),\n",
    "            'label': torch.tensor(row['label'], dtype=torch.float32)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9657b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TRAINING CONFIGURATION\n",
      "============================================================\n",
      "\n",
      "Available models:\n",
      "1. paraphrase-multilingual-MiniLM-L12-v2 (Fast, 118M params)\n",
      "2. xlm-roberta-base (Balanced, 279M params)\n",
      "3. distilbert-base-multilingual-cased (Faster, 135M params)\n",
      "\n",
      "Current configuration:\n",
      "  model_name: sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\n",
      "  pooling: mean\n",
      "  max_length: 128\n",
      "  batch_size: 16\n",
      "  learning_rate: 2e-05\n",
      "  num_epochs: 5\n",
      "  warmup_steps: 5\n",
      "  weight_decay: 0.01\n",
      "  max_grad_norm: 1.0\n",
      "  log_interval: 10\n",
      "  save_steps: 500\n",
      "\n",
      "Configuration set\n",
      "\n",
      "Final configuration:\n",
      "  model_name: sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\n",
      "  pooling: mean\n",
      "  max_length: 128\n",
      "  batch_size: 16\n",
      "  learning_rate: 2e-05\n",
      "  num_epochs: 5\n",
      "  warmup_steps: 5\n",
      "  weight_decay: 0.01\n",
      "  max_grad_norm: 1.0\n",
      "  log_interval: 10\n",
      "  save_steps: 500\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING CONFIGURATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "\n",
    "print(\"\\nAvailable models:\")\n",
    "print(\"1. paraphrase-multilingual-MiniLM-L12-v2 (Fast, 118M params)\")\n",
    "print(\"2. xlm-roberta-base (Balanced, 279M params)\")\n",
    "print(\"3. distilbert-base-multilingual-cased (Faster, 135M params)\")\n",
    "\n",
    "model_choice = input(\"\\nChoose model (1-3, default: 1): \").strip() or \"1\"\n",
    "\n",
    "model_names = {\n",
    "    '1': 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2',\n",
    "    '2': 'xlm-roberta-base',\n",
    "    '3': 'distilbert-base-multilingual-cased'\n",
    "}\n",
    "\n",
    "CONFIG = {\n",
    "    'model_name': model_names.get(model_choice, model_names['1']),\n",
    "    'pooling': 'mean',\n",
    "    'max_length': 128,\n",
    "    'batch_size': 8,  \n",
    "    'learning_rate': 2e-5,\n",
    "    'num_epochs': 2,\n",
    "    'warmup_steps': 2,\n",
    "    'weight_decay': 0.01,\n",
    "    'max_grad_norm': 1.0,\n",
    "    'log_interval': 10,\n",
    "    'save_steps': 500,\n",
    "}\n",
    "\n",
    "print(\"\\nCurrent configuration:\")\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "customize = input(\"\\nCustomize configuration? (y/n, default: n): \").strip().lower()\n",
    "\n",
    "if customize == 'y':\n",
    "    CONFIG['batch_size'] = int(input(f\"Batch size (current: {CONFIG['batch_size']}): \") or CONFIG['batch_size'])\n",
    "    CONFIG['learning_rate'] = float(input(f\"Learning rate (current: {CONFIG['learning_rate']}): \") or CONFIG['learning_rate'])\n",
    "    CONFIG['num_epochs'] = int(input(f\"Number of epochs (current: {CONFIG['num_epochs']}): \") or CONFIG['num_epochs'])\n",
    "\n",
    "print(\"\\nConfiguration set\")\n",
    "print(\"\\nFinal configuration:\")\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7031da42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "INITIALIZING TRAINING\n",
      "============================================================\n",
      "\n",
      " Device: cpu\n",
      "\n",
      "Loading tokenizer...\n",
      " Tokenizer loaded\n",
      "\n",
      " Creating datasets...\n",
      " Datasets created\n",
      "   Train: 69 samples\n",
      "   Val: 15 samples\n",
      "   Test: 16 samples\n",
      " DataLoaders created\n",
      "   Train batches: 5\n",
      "   Val batches: 1\n",
      "   Test batches: 1\n",
      "\n",
      " Initializing model...\n",
      "Loading model: sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Model loaded\n",
      "   Hidden size: 384\n",
      "   Pooling strategy: mean\n",
      " Model initialized\n",
      "   Total parameters: 117,653,760\n",
      "   Trainable parameters: 117,653,760\n",
      " Optimizer: AdamW\n",
      " Scheduler: OneCycleLR\n",
      "   Total steps: 25\n",
      "   Warmup steps: 5\n",
      " Loss: CosineEmbeddingLoss\n"
     ]
    }
   ],
   "source": [
    "# Initialize Training\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"INITIALIZING TRAINING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"\\n Device: {device}\")\n",
    "\n",
    "print(f\"\\nLoading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(CONFIG['model_name'])\n",
    "print(\" Tokenizer loaded\")\n",
    "\n",
    "print(\"\\n Creating datasets...\")\n",
    "train_dataset = MultilingualSearchDataset(train_df, tokenizer, CONFIG['max_length'])\n",
    "val_dataset = MultilingualSearchDataset(val_df, tokenizer, CONFIG['max_length'])\n",
    "test_dataset = MultilingualSearchDataset(test_df, tokenizer, CONFIG['max_length'])\n",
    "\n",
    "print(f\" Datasets created\")\n",
    "print(f\"   Train: {len(train_dataset)} samples\")\n",
    "print(f\"   Val: {len(val_dataset)} samples\")\n",
    "print(f\"   Test: {len(test_dataset)} samples\")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\" DataLoaders created\")\n",
    "print(f\"   Train batches: {len(train_loader)}\")\n",
    "print(f\"   Val batches: {len(val_loader)}\")\n",
    "print(f\"   Test batches: {len(test_loader)}\")\n",
    "\n",
    "print(f\"\\n Initializing model...\")\n",
    "model = BiEncoderModel(CONFIG['model_name'], CONFIG['pooling'])\n",
    "model = model.to(device)\n",
    "\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "num_trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\" Model initialized\")\n",
    "print(f\"   Total parameters: {num_params:,}\")\n",
    "print(f\"   Trainable parameters: {num_trainable:,}\")\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=CONFIG['learning_rate'],\n",
    "    weight_decay=CONFIG['weight_decay']\n",
    ")\n",
    "\n",
    "num_training_steps = len(train_loader) * CONFIG['num_epochs']\n",
    "num_warmup_steps = CONFIG['warmup_steps']\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=CONFIG['learning_rate'],\n",
    "    total_steps=num_training_steps,\n",
    "    pct_start=num_warmup_steps/num_training_steps,\n",
    "    anneal_strategy='cos'\n",
    ")\n",
    "\n",
    "print(f\" Optimizer: AdamW\")\n",
    "print(f\" Scheduler: OneCycleLR\")\n",
    "print(f\"   Total steps: {num_training_steps}\")\n",
    "print(f\"   Warmup steps: {num_warmup_steps}\")\n",
    "\n",
    "criterion = nn.CosineEmbeddingLoss()\n",
    "print(f\" Loss: CosineEmbeddingLoss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "49da4743",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, optimizer, scheduler, criterion, device, epoch, config):\n",
    "  \n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch}\")\n",
    "    \n",
    "    for batch_idx, batch in enumerate(pbar):\n",
    "        # Move to device\n",
    "        query_input_ids = batch['query_input_ids'].to(device)\n",
    "        query_attention_mask = batch['query_attention_mask'].to(device)\n",
    "        doc_input_ids = batch['doc_input_ids'].to(device)\n",
    "        doc_attention_mask = batch['doc_attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        query_emb = model(query_input_ids, query_attention_mask)\n",
    "        doc_emb = model(doc_input_ids, doc_attention_mask)\n",
    "        \n",
    "        # Compute loss\n",
    "        targets = (labels * 2) - 1  # Convert 0/1 to -1/1\n",
    "        loss = criterion(query_emb, doc_emb, targets)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), config['max_grad_norm'])\n",
    "        \n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Track metrics\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        similarities = torch.nn.functional.cosine_similarity(query_emb, doc_emb)\n",
    "        predictions = (similarities > 0.5).float()\n",
    "        correct += (predictions == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        \n",
    "        # Update progress bar\n",
    "        current_lr = scheduler.get_last_lr()[0]\n",
    "        pbar.set_postfix({\n",
    "            'loss': f'{loss.item():.4f}',\n",
    "            'acc': f'{100 * correct / total:.2f}%',\n",
    "            'lr': f'{current_lr:.2e}'\n",
    "        })\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    accuracy = 100 * correct / total\n",
    "    \n",
    "    return avg_loss, accuracy\n",
    "\n",
    "\n",
    "def evaluate(model, data_loader, criterion, device):\n",
    "  \n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    all_similarities = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader, desc=\"Evaluating\"):\n",
    "            query_input_ids = batch['query_input_ids'].to(device)\n",
    "            query_attention_mask = batch['query_attention_mask'].to(device)\n",
    "            doc_input_ids = batch['doc_input_ids'].to(device)\n",
    "            doc_attention_mask = batch['doc_attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            query_emb = model(query_input_ids, query_attention_mask)\n",
    "            doc_emb = model(doc_input_ids, doc_attention_mask)\n",
    "            \n",
    "            # Compute loss\n",
    "            targets = (labels * 2) - 1\n",
    "            loss = criterion(query_emb, doc_emb, targets)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            similarities = torch.nn.functional.cosine_similarity(query_emb, doc_emb)\n",
    "            predictions = (similarities > 0.5).float()\n",
    "            correct += (predictions == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "            \n",
    "            # Store for analysis\n",
    "            all_similarities.extend(similarities.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    accuracy = 100 * correct / total\n",
    "    \n",
    "    # Calculate additional metrics\n",
    "    similarities_np = np.array(all_similarities)\n",
    "    labels_np = np.array(all_labels)\n",
    "    \n",
    "    # True positives, false positives, etc.\n",
    "    tp = np.sum((similarities_np > 0.5) & (labels_np == 1))\n",
    "    fp = np.sum((similarities_np > 0.5) & (labels_np == 0))\n",
    "    tn = np.sum((similarities_np <= 0.5) & (labels_np == 0))\n",
    "    fn = np.sum((similarities_np <= 0.5) & (labels_np == 1))\n",
    "    \n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'loss': avg_loss,\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision * 100,\n",
    "        'recall': recall * 100,\n",
    "        'f1': f1 * 100\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aacd3f34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing MLflow...\n",
      "âœ… MLflow configured\n",
      "   Tracking URI: file:///workspaces/MultiLang_NLP_Sysytem/mlruns\n",
      "   Experiment: multilingual-search-training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/mlflow/tracking/_tracking_service/utils.py:178: FutureWarning: The filesystem tracking backend (e.g., './mlruns') will be deprecated in February 2026. Consider transitioning to a database backend (e.g., 'sqlite:///mlflow.db') to take advantage of the latest MLflow features. See https://github.com/mlflow/mlflow/issues/18534 for more details and migration guidance. For migrating existing data, https://github.com/mlflow/mlflow-export-import can be used.\n",
      "  return FileStore(store_uri, store_uri)\n"
     ]
    }
   ],
   "source": [
    "print(\"Installing MLflow...\")\n",
    "import subprocess\n",
    "import sys\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"mlflow\"])\n",
    "\n",
    "import mlflow\n",
    "\n",
    "# Set MLflow tracking directory (use default or local file store)\n",
    "mlflow.set_tracking_uri(\"file:///workspaces/MultiLang_NLP_Sysytem/mlruns\")\n",
    "\n",
    "# Create or get experiment\n",
    "experiment_name = \"multilingual-search-training\"\n",
    "try:\n",
    "    experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "    if experiment is None:\n",
    "        mlflow.create_experiment(experiment_name)\n",
    "    mlflow.set_experiment(experiment_name)\n",
    "except Exception as e:\n",
    "    print(f\"Note: Using default experiment ({e})\")\n",
    "\n",
    "print(\"âœ… MLflow configured\")\n",
    "print(f\"   Tracking URI: {mlflow.get_tracking_uri()}\")\n",
    "print(f\"   Experiment: {experiment_name}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3394f392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Ready to start training with MLflow tracking\n"
     ]
    }
   ],
   "source": [
    "# Ensure no active runs before starting\n",
    "active_run = mlflow.active_run()\n",
    "if active_run:\n",
    "    mlflow.end_run()\n",
    "    print(f\"âœ… Ended previous run: {active_run.info.run_id}\")\n",
    "\n",
    "print(\"âœ… Ready to start training with MLflow tracking\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e47e0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STARTING TRAINING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'train_acc': [],\n",
    "    'val_loss': [],\n",
    "    'val_acc': [],\n",
    "    'val_precision': [],\n",
    "    'val_recall': [],\n",
    "    'val_f1': [],\n",
    "    'learning_rates': []\n",
    "}\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "best_model_state = None\n",
    "\n",
    "# Start MLflow run\n",
    "mlflow.start_run(run_name=\"training-run\")\n",
    "\n",
    "# Log configuration parameters\n",
    "mlflow.log_params({\n",
    "    'model_name': CONFIG['model_name'],\n",
    "    'batch_size': CONFIG['batch_size'],\n",
    "    'learning_rate': CONFIG['learning_rate'],\n",
    "    'num_epochs': CONFIG['num_epochs'],\n",
    "    'warmup_steps': CONFIG['warmup_steps'],\n",
    "    'weight_decay': CONFIG['weight_decay'],\n",
    "    'max_grad_norm': CONFIG['max_grad_norm'],\n",
    "})\n",
    "\n",
    "try:\n",
    "    for epoch in range(1, CONFIG['num_epochs'] + 1):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Epoch {epoch}/{CONFIG['num_epochs']}\")\n",
    "        print('='*60)\n",
    "        \n",
    "        # Train\n",
    "        train_loss, train_acc = train_epoch(\n",
    "            model, train_loader, optimizer, scheduler, criterion, \n",
    "            device, epoch, CONFIG\n",
    "        )\n",
    "\n",
    "        # Validate\n",
    "        print(\"\\nValidating...\")\n",
    "        val_metrics = evaluate(model, val_loader, criterion, device)\n",
    "\n",
    "        # Track history\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(val_metrics['loss'])\n",
    "        history['val_acc'].append(val_metrics['accuracy'])\n",
    "        history['val_precision'].append(val_metrics['precision'])\n",
    "        history['val_recall'].append(val_metrics['recall'])\n",
    "        history['val_f1'].append(val_metrics['f1'])\n",
    "        history['learning_rates'].append(optimizer.param_groups[0]['lr'])\n",
    "\n",
    "        # Log metrics to MLflow\n",
    "        mlflow.log_metric(\"train_loss\", train_loss, step=epoch)\n",
    "        mlflow.log_metric(\"train_accuracy\", train_acc, step=epoch)\n",
    "        mlflow.log_metric(\"val_loss\", val_metrics['loss'], step=epoch)\n",
    "        mlflow.log_metric(\"val_accuracy\", val_metrics['accuracy'], step=epoch)\n",
    "        mlflow.log_metric(\"val_precision\", val_metrics['precision'], step=epoch)\n",
    "        mlflow.log_metric(\"val_recall\", val_metrics['recall'], step=epoch)\n",
    "        mlflow.log_metric(\"val_f1\", val_metrics['f1'], step=epoch)\n",
    "        mlflow.log_metric(\"learning_rate\", optimizer.param_groups[0]['lr'], step=epoch)\n",
    "\n",
    "        print(f\"\\nðŸ“Š Epoch {epoch} Results:\")\n",
    "        print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%\")\n",
    "        print(f\"  Val Loss: {val_metrics['loss']:.4f} | Val Acc: {val_metrics['accuracy']:.2f}%\")\n",
    "        print(f\"  Val Precision: {val_metrics['precision']:.2f}%\")\n",
    "        print(f\"  Val Recall: {val_metrics['recall']:.2f}%\")\n",
    "        print(f\"  Val F1: {val_metrics['f1']:.2f}%\")\n",
    "\n",
    "        # Save best model\n",
    "        if val_metrics['loss'] < best_val_loss:\n",
    "            best_val_loss = val_metrics['loss']\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            print(f\" New best model! (Val Loss: {best_val_loss:.4f})\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nâŒ Error during training: {e}\")\n",
    "    raise\n",
    "\n",
    "finally:\n",
    "    # End MLflow run\n",
    "    mlflow.end_run()\n",
    "    print(\"\\n MLflow run completed\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING COMPLETED\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
