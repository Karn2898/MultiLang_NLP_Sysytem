{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fd7c463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing dependencies...\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "print(\"Installing dependencies...\")\n",
    "!pip install -q transformers torch sentence-transformers datasets pandas scikit-learn langdetect\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from google.colab import drive, files\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "from typing import List, Dict, Tuple\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf34a60d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "DATASET LOADING OPTIONS\n",
      "============================================================\n",
      "\n",
      "Choose how to load your dataset:\n",
      "1. Upload CSV file\n",
      "2. Upload JSON file\n",
      "3. Load from Google Drive\n",
      "4. Use sample data for testing\n",
      "5. Generate synthetic multilingual data\n",
      "\n",
      " Using sample data\n",
      "\n",
      "üìä Generating 100 synthetic samples...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b60569c5d194183afcc4de237941f26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating data:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Generated 100 samples\n",
      "   Positive pairs: 70 (70.0%)\n",
      "   Negative pairs: 30 (30.0%)\n",
      "   Languages: ['en', 'zh', 'fr', 'it', 'es', 'de']\n",
      "   Domains: ['information', 'travel', 'shopping']\n",
      "\n",
      "============================================================\n",
      "DATASET OVERVIEW\n",
      "============================================================\n",
      "\n",
      "Total samples: 100\n",
      "\n",
      "First few rows:\n",
      "                 query                       doc  label query_lang doc_lang  \\\n",
      "0  where to find learn                  ‰ªÄ‰πàÊòØlearn      1         en       zh   \n",
      "1             ‰∏∫‰ªÄ‰πàlearn                   Â¶Ç‰Ωïlearn      1         zh       zh   \n",
      "2     flights to Paris    hoteles cerca de Paris      1         en       es   \n",
      "3          ‰∏∫‰ªÄ‰πàexercise              ‰ªÄ‰πàÊó∂ÂÄôexercise      1         zh       zh   \n",
      "4     when to meditate  d√≥nde encontrar meditate      1         en       es   \n",
      "\n",
      "        domain  \n",
      "0  information  \n",
      "1  information  \n",
      "2       travel  \n",
      "3  information  \n",
      "4  information  \n",
      "\n",
      "Dataset statistics:\n",
      "                                 query           doc       label query_lang  \\\n",
      "count                              100           100  100.000000        100   \n",
      "unique                              95            96         NaN          6   \n",
      "top     attrazioni turistiche a Mumbai  comment cook         NaN         es   \n",
      "freq                                 2             2         NaN         20   \n",
      "mean                               NaN           NaN    0.700000        NaN   \n",
      "std                                NaN           NaN    0.460566        NaN   \n",
      "min                                NaN           NaN    0.000000        NaN   \n",
      "25%                                NaN           NaN    0.000000        NaN   \n",
      "50%                                NaN           NaN    1.000000        NaN   \n",
      "75%                                NaN           NaN    1.000000        NaN   \n",
      "max                                NaN           NaN    1.000000        NaN   \n",
      "\n",
      "       doc_lang       domain  \n",
      "count       100          100  \n",
      "unique        6            3  \n",
      "top          zh  information  \n",
      "freq         24           40  \n",
      "mean        NaN          NaN  \n",
      "std         NaN          NaN  \n",
      "min         NaN          NaN  \n",
      "25%         NaN          NaN  \n",
      "50%         NaN          NaN  \n",
      "75%         NaN          NaN  \n",
      "max         NaN          NaN  \n",
      "\n",
      "Label distribution:\n",
      "label\n",
      "1    70\n",
      "0    30\n",
      "Name: count, dtype: int64\n",
      " Labels are binary (0/1)\n",
      "\n",
      "üßπ Cleaning data...\n",
      "   Removed 0 invalid rows\n",
      "   Final dataset size: 100\n",
      "\n",
      " Cleaned dataset saved to: /content/cleaned_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DATASET LOADING OPTIONS\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nChoose how to load your dataset:\")\n",
    "print(\"1. Upload CSV file\")\n",
    "print(\"2. Upload JSON file\")\n",
    "print(\"3. Load from Google Drive\")\n",
    "print(\"4. Use sample data for testing\")\n",
    "print(\"5. Generate synthetic multilingual data\")\n",
    "\n",
    "dataset_choice = input(\"\\nEnter choice (1-5): \").strip()\n",
    "\n",
    "def load_csv_dataset(file_path):\n",
    "    \"\"\"Load dataset from CSV\"\"\"\n",
    "    df = pd.read_csv(file_path)\n",
    "    print(f\"\\n Loaded CSV with {len(df)} rows\")\n",
    "    print(f\"   Columns: {list(df.columns)}\")\n",
    "    return df\n",
    "\n",
    "def load_json_dataset(file_path):\n",
    "   \n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    if isinstance(data, list):\n",
    "        df = pd.DataFrame(data)\n",
    "    else:\n",
    "        df = pd.DataFrame([data])\n",
    "    \n",
    "    print(f\"\\n Loaded JSON with {len(df)} rows\")\n",
    "    print(f\"   Columns: {list(df.columns)}\")\n",
    "    return df\n",
    "\n",
    "def generate_synthetic_data(num_samples=1000):\n",
    "   \n",
    "    print(f\"\\nüìä Generating {num_samples} synthetic samples...\")\n",
    "\n",
    "    templates = {\n",
    "        'travel': {\n",
    "            'en': ['cheap hotels in {}', 'flights to {}', 'tourist attractions in {}', \n",
    "                   'restaurants in {}', 'hotels near {}'],\n",
    "            'es': ['hoteles baratos en {}', 'vuelos a {}', 'atracciones tur√≠sticas en {}',\n",
    "                   'restaurantes en {}', 'hoteles cerca de {}'],\n",
    "            'fr': ['h√¥tels bon march√© √† {}', 'vols vers {}', 'attractions touristiques √† {}',\n",
    "                   'restaurants √† {}', 'h√¥tels pr√®s de {}'],\n",
    "            'de': ['g√ºnstige Hotels in {}', 'Fl√ºge nach {}', 'Touristenattraktionen in {}',\n",
    "                   'Restaurants in {}', 'Hotels in der N√§he von {}'],\n",
    "            'it': ['hotel economici a {}', 'voli per {}', 'attrazioni turistiche a {}',\n",
    "                   'ristoranti a {}', 'hotel vicino a {}'],\n",
    "            'zh': ['{}ÁöÑ‰æøÂÆúÈÖíÂ∫ó', 'È£ûÂæÄ{}', '{}ÁöÑÊóÖÊ∏∏ÊôØÁÇπ', '{}ÁöÑÈ§êÂéÖ', '{}ÈôÑËøëÁöÑÈÖíÂ∫ó'],\n",
    "        },\n",
    "        'shopping': {\n",
    "            'en': ['buy {} online', 'best {} deals', '{} for sale', 'cheap {}', 'discount {}'],\n",
    "            'es': ['comprar {} en l√≠nea', 'mejores ofertas de {}', '{} en venta', '{} barato', '{} con descuento'],\n",
    "            'fr': ['acheter {} en ligne', 'meilleures offres {}', '{} √† vendre', '{} pas cher', '{} en promotion'],\n",
    "            'de': ['{} online kaufen', 'beste {} Angebote', '{} zu verkaufen', 'g√ºnstige {}', '{} im Angebot'],\n",
    "            'it': ['acquista {} online', 'migliori offerte {}', '{} in vendita', '{} economico', '{} in sconto'],\n",
    "            'zh': ['Âú®Á∫øË¥≠‰π∞{}', 'ÊúÄÂ•ΩÁöÑ{}‰ºòÊÉ†', '{}Âá∫ÂîÆ', '‰æøÂÆúÁöÑ{}', '{}ÊäòÊâ£'],\n",
    "        },\n",
    "        'information': {\n",
    "            'en': ['how to {}', 'what is {}', 'where to find {}', 'when to {}', 'why {}'],\n",
    "            'es': ['c√≥mo {}', 'qu√© es {}', 'd√≥nde encontrar {}', 'cu√°ndo {}', 'por qu√© {}'],\n",
    "            'fr': ['comment {}', 'qu\\'est-ce que {}', 'o√π trouver {}', 'quand {}', 'pourquoi {}'],\n",
    "            'de': ['wie man {}', 'was ist {}', 'wo finde ich {}', 'wann {}', 'warum {}'],\n",
    "            'it': ['come {}', 'cos\\'√® {}', 'dove trovare {}', 'quando {}', 'perch√© {}'],\n",
    "            'zh': ['Â¶Ç‰Ωï{}', '‰ªÄ‰πàÊòØ{}', 'Âú®Âì™ÈáåÊâæÂà∞{}', '‰ªÄ‰πàÊó∂ÂÄô{}', '‰∏∫‰ªÄ‰πà{}'],\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    cities = ['Paris', 'London', 'Tokyo', 'New York', 'Rome', 'Berlin', 'Madrid', \n",
    "              'Barcelona', 'Amsterdam', 'Dubai', 'Singapore', 'Mumbai', 'Beijing']\n",
    "    \n",
    "    products = ['laptop', 'phone', 'camera', 'shoes', 'watch', 'book', 'tablet', \n",
    "                'headphones', 'backpack', 'jacket']\n",
    "    \n",
    "    topics = ['cook', 'learn', 'travel', 'exercise', 'meditate', 'study', 'work', \n",
    "              'relax', 'save money', 'start business']\n",
    "    \n",
    "    data = []\n",
    "    languages = list(templates['travel'].keys())\n",
    "    \n",
    "    for _ in tqdm(range(num_samples), desc=\"Generating data\"):\n",
    "        domain = random.choice(list(templates.keys()))\n",
    "        \n",
    "      \n",
    "        if domain == 'travel':\n",
    "            entity = random.choice(cities)\n",
    "        elif domain == 'shopping':\n",
    "            entity = random.choice(products)\n",
    "        else:\n",
    "            entity = random.choice(topics)\n",
    "        \n",
    "      \n",
    "        lang1 = random.choice(languages)\n",
    "        lang2 = random.choice(languages)\n",
    "        \n",
    "        template1 = random.choice(templates[domain][lang1])\n",
    "        template2 = random.choice(templates[domain][lang2])\n",
    "        \n",
    "        query = template1.format(entity)\n",
    "        \n",
    "\n",
    "        if random.random() < 0.7:\n",
    "            doc = template2.format(entity)\n",
    "            label = 1\n",
    "        else:\n",
    "    \n",
    "            if random.random() < 0.5:\n",
    "       \n",
    "                other_entity = random.choice(cities if domain == 'travel' else \n",
    "                                            products if domain == 'shopping' else topics)\n",
    "                while other_entity == entity:\n",
    "                    other_entity = random.choice(cities if domain == 'travel' else \n",
    "                                                products if domain == 'shopping' else topics)\n",
    "                doc = template2.format(other_entity)\n",
    "            else:\n",
    "              \n",
    "                other_domain = random.choice([d for d in templates.keys() if d != domain])\n",
    "                other_template = random.choice(templates[other_domain][lang2])\n",
    "                other_entity = random.choice(cities if other_domain == 'travel' else \n",
    "                                            products if other_domain == 'shopping' else topics)\n",
    "                doc = other_template.format(other_entity)\n",
    "            label = 0\n",
    "        \n",
    "        data.append({\n",
    "            'query': query,\n",
    "            'doc': doc,\n",
    "            'label': label,\n",
    "            'query_lang': lang1,\n",
    "            'doc_lang': lang2,\n",
    "            'domain': domain\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    print(f\"\\n Generated {len(df)} samples\")\n",
    "    print(f\"   Positive pairs: {(df['label']==1).sum()} ({(df['label']==1).sum()/len(df)*100:.1f}%)\")\n",
    "    print(f\"   Negative pairs: {(df['label']==0).sum()} ({(df['label']==0).sum()/len(df)*100:.1f}%)\")\n",
    "    print(f\"   Languages: {df['query_lang'].unique().tolist()}\")\n",
    "    print(f\"   Domains: {df['domain'].unique().tolist()}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "if dataset_choice == '1':\n",
    "    print(\"\\n Upload your CSV file\")\n",
    "    print(\"Expected format: columns [query, doc, label]\")\n",
    "    print(\"  - query: search query text\")\n",
    "    print(\"  - doc: document text\")\n",
    "    print(\"  - label: 1 (similar) or 0 (not similar)\")\n",
    "    uploaded = files.upload()\n",
    "    file_path = list(uploaded.keys())[0]\n",
    "    df = load_csv_dataset(file_path)\n",
    "    \n",
    "elif dataset_choice == '2':\n",
    "    print(\"\\n Upload your JSON file\")\n",
    "    print(\"Expected format: list of {query, doc, label}\")\n",
    "    uploaded = files.upload()\n",
    "    file_path = list(uploaded.keys())[0]\n",
    "    df = load_json_dataset(file_path)\n",
    "    \n",
    "elif dataset_choice == '3':\n",
    "    file_path = input(\"\\nEnter Google Drive path (e.g., /content/drive/MyDrive/data.csv): \").strip()\n",
    "    if file_path.endswith('.csv'):\n",
    "        df = load_csv_dataset(file_path)\n",
    "    elif file_path.endswith('.json'):\n",
    "        df = load_json_dataset(file_path)\n",
    "    else:\n",
    "        print(\"Unsupported file format. Using sample data.\")\n",
    "        df = generate_synthetic_data(100)\n",
    "        \n",
    "elif dataset_choice == '5':\n",
    "    num_samples = int(input(\"\\nHow many samples to generate? (default: 1000): \").strip() or \"1000\")\n",
    "    df = generate_synthetic_data(num_samples)\n",
    "    \n",
    "else: \n",
    "    print(\"\\n Using sample data\")\n",
    "    df = generate_synthetic_data(100)\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DATASET OVERVIEW\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nTotal samples: {len(df)}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(df.head())\n",
    "\n",
    "print(f\"\\nDataset statistics:\")\n",
    "print(df.describe(include='all'))\n",
    "\n",
    "\n",
    "required_cols = ['query', 'doc', 'label']\n",
    "missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "\n",
    "if missing_cols:\n",
    "    print(f\"\\n  Missing columns: {missing_cols}\")\n",
    "    print(\"Attempting to auto-detect columns...\")\n",
    "    \n",
    "  \n",
    "    for col in missing_cols:\n",
    "        possible_names = [c for c in df.columns if col.lower() in c.lower()]\n",
    "        if possible_names:\n",
    "            df.rename(columns={possible_names[0]: col}, inplace=True)\n",
    "            print(f\"   Renamed '{possible_names[0]}' ‚Üí '{col}'\")\n",
    "\n",
    "\n",
    "if 'label' in df.columns:\n",
    "    unique_labels = df['label'].unique()\n",
    "    print(f\"\\nLabel distribution:\")\n",
    "    print(df['label'].value_counts())\n",
    "    \n",
    "\n",
    "    if set(unique_labels).issubset({0, 1}):\n",
    "        print(\" Labels are binary (0/1)\")\n",
    "    else:\n",
    "        print(f\"Found labels: {unique_labels}\")\n",
    "        print(\"Converting to binary (0/1)...\")\n",
    "        df['label'] = df['label'].apply(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "\n",
    "print(\"\\nüßπ Cleaning data...\")\n",
    "initial_len = len(df)\n",
    "df = df.dropna(subset=['query', 'doc', 'label'])\n",
    "df = df[df['query'].str.strip() != '']\n",
    "df = df[df['doc'].str.strip() != '']\n",
    "print(f\"   Removed {initial_len - len(df)} invalid rows\")\n",
    "print(f\"   Final dataset size: {len(df)}\")\n",
    "\n",
    "\n",
    "cleaned_path = '/content/cleaned_dataset.csv'\n",
    "df.to_csv(cleaned_path, index=False)\n",
    "print(f\"\\n Cleaned dataset saved to: {cleaned_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d006928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "DATA SPLITTING\n",
      "============================================================\n",
      "\n",
      "Split ratios:\n",
      "  Train: 70.0%\n",
      "  Validation: 15.0%\n",
      "  Test: 15.0%\n",
      "\n",
      "Dataset sizes:\n",
      "  Train: 69 samples\n",
      "  Validation: 15 samples\n",
      "  Test: 16 samples\n",
      "\n",
      "Label distribution:\n",
      "  Train - Positive: 48, Negative: 21\n",
      "  Val   - Positive: 11, Negative: 4\n",
      "  Test  - Positive: 11, Negative: 5\n",
      "\n",
      " Data splits saved\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DATA SPLITTING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "\n",
    "train_ratio = 0.7\n",
    "val_ratio = 0.15\n",
    "test_ratio = 0.15\n",
    "\n",
    "print(f\"\\nSplit ratios:\")\n",
    "print(f\"  Train: {train_ratio*100}%\")\n",
    "print(f\"  Validation: {val_ratio*100}%\")\n",
    "print(f\"  Test: {test_ratio*100}%\")\n",
    "\n",
    "\n",
    "train_df, temp_df = train_test_split(\n",
    "    df, \n",
    "    test_size=(1 - train_ratio),\n",
    "    stratify=df['label'],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "val_df, test_df = train_test_split(\n",
    "    temp_df,\n",
    "    test_size=test_ratio/(test_ratio + val_ratio),\n",
    "    stratify=temp_df['label'],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"\\nDataset sizes:\")\n",
    "print(f\"  Train: {len(train_df)} samples\")\n",
    "print(f\"  Validation: {len(val_df)} samples\")\n",
    "print(f\"  Test: {len(test_df)} samples\")\n",
    "\n",
    "\n",
    "print(f\"\\nLabel distribution:\")\n",
    "print(f\"  Train - Positive: {(train_df['label']==1).sum()}, Negative: {(train_df['label']==0).sum()}\")\n",
    "print(f\"  Val   - Positive: {(val_df['label']==1).sum()}, Negative: {(val_df['label']==0).sum()}\")\n",
    "print(f\"  Test  - Positive: {(test_df['label']==1).sum()}, Negative: {(test_df['label']==0).sum()}\")\n",
    "\n",
    "\n",
    "train_df.to_csv('/content/train.csv', index=False)\n",
    "val_df.to_csv('/content/val.csv', index=False)\n",
    "test_df.to_csv('/content/test.csv', index=False)\n",
    "\n",
    "print(\"\\n Data splits saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "618aaf16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Model class defined\n"
     ]
    }
   ],
   "source": [
    "class BiEncoderModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2', \n",
    "                 pooling='mean'):\n",
    "        super().__init__()\n",
    "        \n",
    "        print(f\"Loading model: {model_name}\")\n",
    "        self.encoder = AutoModel.from_pretrained(model_name)\n",
    "        self.pooling = pooling\n",
    "        self.config = self.encoder.config\n",
    "        \n",
    "        print(f\"‚úÖ Model loaded\")\n",
    "        print(f\"   Hidden size: {self.config.hidden_size}\")\n",
    "        print(f\"   Pooling strategy: {pooling}\")\n",
    "        \n",
    "    def mean_pooling(self, token_embeddings, attention_mask):\n",
    "        \n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "        sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "        return sum_embeddings / sum_mask\n",
    "    \n",
    "    def cls_pooling(self, token_embeddings):\n",
    "     \n",
    "        return token_embeddings[:, 0, :]\n",
    "    \n",
    "    def max_pooling(self, token_embeddings, attention_mask):\n",
    "  \n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        token_embeddings[input_mask_expanded == 0] = -1e9\n",
    "        return torch.max(token_embeddings, 1)[0]\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "     \n",
    "        \n",
    "        outputs = self.encoder(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            return_dict=True\n",
    "        )\n",
    "        \n",
    "       \n",
    "        if self.pooling == 'mean':\n",
    "            embeddings = self.mean_pooling(outputs.last_hidden_state, attention_mask)\n",
    "        elif self.pooling == 'cls':\n",
    "            embeddings = self.cls_pooling(outputs.last_hidden_state)\n",
    "        elif self.pooling == 'max':\n",
    "            embeddings = self.max_pooling(outputs.last_hidden_state, attention_mask)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown pooling: {self.pooling}\")\n",
    "        \n",
    "      \n",
    "        embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)\n",
    "        \n",
    "        return embeddings\n",
    "\n",
    "print(\"\\n Model class defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5a6c4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultilingualSearchDataset(Dataset):\n",
    "   \n",
    "    \n",
    "    def __init__(self, dataframe, tokenizer, max_length=128):\n",
    "        self.data = dataframe.reset_index(drop=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        \n",
    "      \n",
    "        query_encoding = self.tokenizer(\n",
    "            str(row['query']),\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "       \n",
    "        doc_encoding = self.tokenizer(\n",
    "            str(row['doc']),\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'query_input_ids': query_encoding['input_ids'].squeeze(0),\n",
    "            'query_attention_mask': query_encoding['attention_mask'].squeeze(0),\n",
    "            'doc_input_ids': doc_encoding['input_ids'].squeeze(0),\n",
    "            'doc_attention_mask': doc_encoding['attention_mask'].squeeze(0),\n",
    "            'label': torch.tensor(row['label'], dtype=torch.float32)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e9657b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TRAINING CONFIGURATION\n",
      "============================================================\n",
      "\n",
      "Available models:\n",
      "1. paraphrase-multilingual-MiniLM-L12-v2 (Fast, 118M params)\n",
      "2. xlm-roberta-base (Balanced, 279M params)\n",
      "3. distilbert-base-multilingual-cased (Faster, 135M params)\n",
      "\n",
      "Current configuration:\n",
      "  model_name: sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\n",
      "  pooling: mean\n",
      "  max_length: 128\n",
      "  batch_size: 16\n",
      "  learning_rate: 2e-05\n",
      "  num_epochs: 5\n",
      "  warmup_steps: 5\n",
      "  weight_decay: 0.01\n",
      "  max_grad_norm: 1.0\n",
      "  log_interval: 10\n",
      "  save_steps: 500\n",
      "\n",
      "Configuration set\n",
      "\n",
      "Final configuration:\n",
      "  model_name: sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\n",
      "  pooling: mean\n",
      "  max_length: 128\n",
      "  batch_size: 16\n",
      "  learning_rate: 2e-05\n",
      "  num_epochs: 5\n",
      "  warmup_steps: 5\n",
      "  weight_decay: 0.01\n",
      "  max_grad_norm: 1.0\n",
      "  log_interval: 10\n",
      "  save_steps: 500\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING CONFIGURATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "\n",
    "print(\"\\nAvailable models:\")\n",
    "print(\"1. paraphrase-multilingual-MiniLM-L12-v2 (Fast, 118M params)\")\n",
    "print(\"2. xlm-roberta-base (Balanced, 279M params)\")\n",
    "print(\"3. distilbert-base-multilingual-cased (Faster, 135M params)\")\n",
    "\n",
    "model_choice = input(\"\\nChoose model (1-3, default: 1): \").strip() or \"1\"\n",
    "\n",
    "model_names = {\n",
    "    '1': 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2',\n",
    "    '2': 'xlm-roberta-base',\n",
    "    '3': 'distilbert-base-multilingual-cased'\n",
    "}\n",
    "\n",
    "CONFIG = {\n",
    "    'model_name': model_names.get(model_choice, model_names['1']),\n",
    "    'pooling': 'mean',\n",
    "    'max_length': 128,\n",
    "    'batch_size': 16,  \n",
    "    'learning_rate': 2e-5,\n",
    "    'num_epochs': 5,\n",
    "    'warmup_steps': 5,\n",
    "    'weight_decay': 0.01,\n",
    "    'max_grad_norm': 1.0,\n",
    "    'log_interval': 10,\n",
    "    'save_steps': 500,\n",
    "}\n",
    "\n",
    "print(\"\\nCurrent configuration:\")\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "customize = input(\"\\nCustomize configuration? (y/n, default: n): \").strip().lower()\n",
    "\n",
    "if customize == 'y':\n",
    "    CONFIG['batch_size'] = int(input(f\"Batch size (current: {CONFIG['batch_size']}): \") or CONFIG['batch_size'])\n",
    "    CONFIG['learning_rate'] = float(input(f\"Learning rate (current: {CONFIG['learning_rate']}): \") or CONFIG['learning_rate'])\n",
    "    CONFIG['num_epochs'] = int(input(f\"Number of epochs (current: {CONFIG['num_epochs']}): \") or CONFIG['num_epochs'])\n",
    "\n",
    "print(\"\\nConfiguration set\")\n",
    "print(\"\\nFinal configuration:\")\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7031da42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "INITIALIZING TRAINING\n",
      "============================================================\n",
      "\n",
      " Device: cpu\n",
      "\n",
      "Loading tokenizer...\n",
      " Tokenizer loaded\n",
      "\n",
      " Creating datasets...\n",
      " Datasets created\n",
      "   Train: 69 samples\n",
      "   Val: 15 samples\n",
      "   Test: 16 samples\n",
      " DataLoaders created\n",
      "   Train batches: 5\n",
      "   Val batches: 1\n",
      "   Test batches: 1\n",
      "\n",
      " Initializing model...\n",
      "Loading model: sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\n",
      "‚úÖ Model loaded\n",
      "   Hidden size: 384\n",
      "   Pooling strategy: mean\n",
      " Model initialized\n",
      "   Total parameters: 117,653,760\n",
      "   Trainable parameters: 117,653,760\n",
      " Optimizer: AdamW\n",
      " Scheduler: OneCycleLR\n",
      "   Total steps: 25\n",
      "   Warmup steps: 5\n",
      " Loss: CosineEmbeddingLoss\n"
     ]
    }
   ],
   "source": [
    "# Initialize Training\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"INITIALIZING TRAINING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"\\n Device: {device}\")\n",
    "\n",
    "print(f\"\\nLoading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(CONFIG['model_name'])\n",
    "print(\" Tokenizer loaded\")\n",
    "\n",
    "print(\"\\n Creating datasets...\")\n",
    "train_dataset = MultilingualSearchDataset(train_df, tokenizer, CONFIG['max_length'])\n",
    "val_dataset = MultilingualSearchDataset(val_df, tokenizer, CONFIG['max_length'])\n",
    "test_dataset = MultilingualSearchDataset(test_df, tokenizer, CONFIG['max_length'])\n",
    "\n",
    "print(f\" Datasets created\")\n",
    "print(f\"   Train: {len(train_dataset)} samples\")\n",
    "print(f\"   Val: {len(val_dataset)} samples\")\n",
    "print(f\"   Test: {len(test_dataset)} samples\")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\" DataLoaders created\")\n",
    "print(f\"   Train batches: {len(train_loader)}\")\n",
    "print(f\"   Val batches: {len(val_loader)}\")\n",
    "print(f\"   Test batches: {len(test_loader)}\")\n",
    "\n",
    "print(f\"\\n Initializing model...\")\n",
    "model = BiEncoderModel(CONFIG['model_name'], CONFIG['pooling'])\n",
    "model = model.to(device)\n",
    "\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "num_trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\" Model initialized\")\n",
    "print(f\"   Total parameters: {num_params:,}\")\n",
    "print(f\"   Trainable parameters: {num_trainable:,}\")\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=CONFIG['learning_rate'],\n",
    "    weight_decay=CONFIG['weight_decay']\n",
    ")\n",
    "\n",
    "num_training_steps = len(train_loader) * CONFIG['num_epochs']\n",
    "num_warmup_steps = CONFIG['warmup_steps']\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=CONFIG['learning_rate'],\n",
    "    total_steps=num_training_steps,\n",
    "    pct_start=num_warmup_steps/num_training_steps,\n",
    "    anneal_strategy='cos'\n",
    ")\n",
    "\n",
    "print(f\" Optimizer: AdamW\")\n",
    "print(f\" Scheduler: OneCycleLR\")\n",
    "print(f\"   Total steps: {num_training_steps}\")\n",
    "print(f\"   Warmup steps: {num_warmup_steps}\")\n",
    "\n",
    "criterion = nn.CosineEmbeddingLoss()\n",
    "print(f\" Loss: CosineEmbeddingLoss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "49da4743",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, optimizer, scheduler, criterion, device, epoch, config):\n",
    "  \n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch}\")\n",
    "    \n",
    "    for batch_idx, batch in enumerate(pbar):\n",
    "        # Move to device\n",
    "        query_input_ids = batch['query_input_ids'].to(device)\n",
    "        query_attention_mask = batch['query_attention_mask'].to(device)\n",
    "        doc_input_ids = batch['doc_input_ids'].to(device)\n",
    "        doc_attention_mask = batch['doc_attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        query_emb = model(query_input_ids, query_attention_mask)\n",
    "        doc_emb = model(doc_input_ids, doc_attention_mask)\n",
    "        \n",
    "        # Compute loss\n",
    "        targets = (labels * 2) - 1  # Convert 0/1 to -1/1\n",
    "        loss = criterion(query_emb, doc_emb, targets)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), config['max_grad_norm'])\n",
    "        \n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Track metrics\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        similarities = torch.nn.functional.cosine_similarity(query_emb, doc_emb)\n",
    "        predictions = (similarities > 0.5).float()\n",
    "        correct += (predictions == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        \n",
    "        # Update progress bar\n",
    "        current_lr = scheduler.get_last_lr()[0]\n",
    "        pbar.set_postfix({\n",
    "            'loss': f'{loss.item():.4f}',\n",
    "            'acc': f'{100 * correct / total:.2f}%',\n",
    "            'lr': f'{current_lr:.2e}'\n",
    "        })\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    accuracy = 100 * correct / total\n",
    "    \n",
    "    return avg_loss, accuracy\n",
    "\n",
    "\n",
    "def evaluate(model, data_loader, criterion, device):\n",
    "  \n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    all_similarities = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader, desc=\"Evaluating\"):\n",
    "            query_input_ids = batch['query_input_ids'].to(device)\n",
    "            query_attention_mask = batch['query_attention_mask'].to(device)\n",
    "            doc_input_ids = batch['doc_input_ids'].to(device)\n",
    "            doc_attention_mask = batch['doc_attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            query_emb = model(query_input_ids, query_attention_mask)\n",
    "            doc_emb = model(doc_input_ids, doc_attention_mask)\n",
    "            \n",
    "            # Compute loss\n",
    "            targets = (labels * 2) - 1\n",
    "            loss = criterion(query_emb, doc_emb, targets)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            similarities = torch.nn.functional.cosine_similarity(query_emb, doc_emb)\n",
    "            predictions = (similarities > 0.5).float()\n",
    "            correct += (predictions == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "            \n",
    "            # Store for analysis\n",
    "            all_similarities.extend(similarities.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    accuracy = 100 * correct / total\n",
    "    \n",
    "    # Calculate additional metrics\n",
    "    similarities_np = np.array(all_similarities)\n",
    "    labels_np = np.array(all_labels)\n",
    "    \n",
    "    # True positives, false positives, etc.\n",
    "    tp = np.sum((similarities_np > 0.5) & (labels_np == 1))\n",
    "    fp = np.sum((similarities_np > 0.5) & (labels_np == 0))\n",
    "    tn = np.sum((similarities_np <= 0.5) & (labels_np == 0))\n",
    "    fn = np.sum((similarities_np <= 0.5) & (labels_np == 1))\n",
    "    \n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'loss': avg_loss,\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision * 100,\n",
    "        'recall': recall * 100,\n",
    "        'f1': f1 * 100\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8e47e0dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STARTING TRAINING\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Epoch 1/5\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Epoch 2/5\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Epoch 3/5\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Epoch 4/5\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Epoch 5/5\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21d9b602741445949c07fcafefe64a5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validating...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f559ed5e2f34611bf1bdc727bb4256a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Epoch 5 Results:\n",
      "  Train Loss: 0.2554 | Train Acc: 94.20%\n",
      "  Val Loss: 0.2168 | Val Acc: 86.67%\n",
      "  Val Precision: 90.91%\n",
      "  Val Recall: 90.91%\n",
      "  Val F1: 90.91%\n",
      " New best model! (Val Loss: 0.2168)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STARTING TRAINING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'train_acc': [],\n",
    "    'val_loss': [],\n",
    "    'val_acc': [],\n",
    "    'val_precision': [],\n",
    "    'val_recall': [],\n",
    "    'val_f1': [],\n",
    "    'learning_rates': []\n",
    "}\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "best_model_state = None\n",
    "\n",
    "\n",
    "for epoch in range(1, CONFIG['num_epochs'] + 1):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Epoch {epoch}/{CONFIG['num_epochs']}\")\n",
    "    print('='*60)\n",
    "# Train\n",
    "train_loss, train_acc = train_epoch(\n",
    "    model, train_loader, optimizer, scheduler, criterion, \n",
    "    device, epoch, CONFIG\n",
    ")\n",
    "\n",
    "\n",
    "print(\"\\nValidating...\")\n",
    "val_metrics = evaluate(model, val_loader, criterion, device)\n",
    "\n",
    "\n",
    "history['train_loss'].append(train_loss)\n",
    "history['train_acc'].append(train_acc)\n",
    "history['val_loss'].append(val_metrics['loss'])\n",
    "history['val_acc'].append(val_metrics['accuracy'])\n",
    "history['val_precision'].append(val_metrics['precision'])\n",
    "history['val_recall'].append(val_metrics['recall'])\n",
    "history['val_f1'].append(val_metrics['f1'])\n",
    "history['learning_rates'].append(optimizer.param_groups[0]['lr'])\n",
    "\n",
    "print(f\"\\nüìä Epoch {epoch} Results:\")\n",
    "print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%\")\n",
    "print(f\"  Val Loss: {val_metrics['loss']:.4f} | Val Acc: {val_metrics['accuracy']:.2f}%\")\n",
    "print(f\"  Val Precision: {val_metrics['precision']:.2f}%\")\n",
    "print(f\"  Val Recall: {val_metrics['recall']:.2f}%\")\n",
    "print(f\"  Val F1: {val_metrics['f1']:.2f}%\")\n",
    "\n",
    "\n",
    "if val_metrics['loss'] < best_val_loss:\n",
    "    best_val_loss = val_metrics['loss']\n",
    "    best_model_state = model.state_dict().copy()\n",
    "    print(f\" New best model! (Val Loss: {best_val_loss:.4f})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
